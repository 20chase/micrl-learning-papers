# MICRL Reading Lists

## 0. Convolutional Neural Networks
### Object classification
**[1]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. **Imagenet classification with deep convolutional neural networks.** Advances in neural information processing systems. 2012.

**[2]** Simonyan, Karen, and Andrew Zisserman. **Very deep convolutional networks for large-scale image recognition.** arXiv preprint arXiv:1409.1556 (2014).

**[3]** Szegedy, Christian, et al. **Going deeper with convolutions.** Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.

**[4]** He, Kaiming, et al. **Deep residual learning for image recognition.** Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.

**[5]** Szegedy, Christian, et al. **Rethinking the inception architecture for computer vision.** Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.

**[6]** Szegedy, Christian, et al. **Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.** AAAI. 2017.

**[7]** Howard, Andrew G., et al. **Mobilenets: Efficient convolutional neural networks for mobile vision applications.** arXiv preprint arXiv:1704.04861 (2017).

**[8]** Sandler, Mark, et al. **Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation.** arXiv preprint arXiv:1801.04381 (2018).

**[9]** Zoph, Barret, et al. **Learning transferable architectures for scalable image recognition.** arXiv preprint arXiv:1707.07012 (2017).

## 1. Deep Reinforcement Learning
### Q Learning
**[1]** Mnih, Volodymyr, et al. **Playing atari with deep reinforcement learning.** arXiv preprint arXiv:1312.5602 (2013).

**[2]** Mnih, Volodymyr, et al. **Human-level control through deep reinforcement learning.** Nature 518.7540 (2015): 529-533.

**[3]** Van Hasselt, Hado, Arthur Guez, and David Silver. **Deep Reinforcement Learning with Double Q-Learning.** AAAI. 2016.

**[4]** Wang, Ziyu, et al. **Dueling network architectures for deep reinforcement learning.** arXiv preprint arXiv:1511.06581 (2015).

**[5]** Schaul, Tom, et al. **Prioritized experience replay.** arXiv preprint arXiv:1511.05952 (2015).

**[6]** He, Frank S., et al. **Learning to play in a day: Faster deep reinforcement learning by optimality tightening.** arXiv preprint arXiv:1611.01606 (2016).

**[7]** Hessel, Matteo, et al. **Rainbow: Combining Improvements in Deep Reinforcement Learning.** arXiv preprint arXiv:1710.02298 (2017).

### Policy Gradient
**[1]** Schulman, John, et al. **Trust region policy optimization.** Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 2015.

**[2]** Schulman, John, et al. **High-dimensional continuous control using generalized advantage estimation.** arXiv preprint arXiv:1506.02438 (2015).

**[3]** Hester, Todd, et al. **Learning from Demonstrations for Real World Reinforcement Learning.** arXiv preprint arXiv:1704.03732 (2017).

**[4]** Mnih, Volodymyr, et al. **Asynchronous methods for deep reinforcement learning.** International Conference on Machine Learning. 2016.

**[5]** Gu, Shixiang, et al. **Q-prop: Sample-efficient policy gradient with an off-policy critic.** arXiv preprint arXiv:1611.02247 (2016).

**[6]** Wang, Ziyu, et al. **Sample efficient actor-critic with experience replay.** arXiv preprint arXiv:1611.01224 (2016).

**[7]** Heess, Nicolas, et al. **Emergence of locomotion behaviours in rich environments.** arXiv preprint arXiv:1707.02286 (2017).

**[8]** Nachum, Ofir, et al. **Trust-PCL: An Off-Policy Trust Region Method for Continuous Control.** arXiv preprint arXiv:1707.01891 (2017).

**[9]** Schulman, John, et al. **Proximal policy optimization algorithms.** arXiv preprint arXiv:1707.06347 (2017).

**[10]** Wu, Yuhuai, et al. **Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.** Advances in Neural Information Processing Systems. 2017.

### Hierarchical Deep Reinforcement Learning
**[1]** Kulkarni, Tejas D., et al. **Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.** Advances in Neural Information Processing Systems. 2016.

**[2]** Bacon, Pierre-Luc, Jean Harb, and Doina Precup. **The Option-Critic Architecture.** AAAI. 2017.

**[3]** Vezhnevets, Alexander Sasha, et al. **Feudal networks for hierarchical reinforcement learning.** arXiv preprint arXiv:1703.01161 (2017).

### Imitation Learning
**[1]** Ho, Jonathan, and Stefano Ermon. **Generative adversarial imitation learning.** Advances in Neural Information Processing Systems. 2016.

### Meta Learning For DRL
**[1]** Al-Shedivat, Maruan, et al. **Continuous adaptation via meta-learning in nonstationary and competitive environments.** arXiv preprint arXiv:1710.03641 (2017).

**[2]** Finn, Chelsea, Pieter Abbeel, and Sergey Levine. **Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.** arXiv preprint arXiv:1703.03400 (2017).

**[3]** Frans, Kevin, et al. **Meta Learning Shared Hierarchies.** arXiv preprint arXiv:1710.09767 (2017).

### Learning From Demonstrations
**[1]** Hester, Todd, et al. **Learning from Demonstrations for Real World Reinforcement Learning.** arXiv preprint arXiv:1704.03732 (2017).

**[2]** Večerík, Matej, et al. **Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.** arXiv preprint arXiv:1707.08817 (2017).

### Others
**[1]** Arulkumaran, Kai, et al. **A Brief Survey of Deep Reinforcement Learning.** (2017).

**[2]** Duan, Yan, et al. **Benchmarking deep reinforcement learning for continuous control.** International Conference on Machine Learning. 2016.

**[3]** Attia, Alexandre, and Sharone Dayan. **Global overview of Imitation Learning.** arXiv preprint arXiv:1801.06503 (2018).

**[4]** Henderson, Peter, et al. **Deep reinforcement learning that matters.** arXiv preprint arXiv:1709.06560 (2017).





